#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœºå™¨å­¦ä¹ è¶‹åŠ¿è¯†åˆ«ä¼˜åŒ–ç³»ç»Ÿ
ä½¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æå‡è¶‹åŠ¿è¯†åˆ«å‡†ç¡®ç‡
"""

import pandas as pd
import numpy as np
import talib
import pickle
import os
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ æ¨¡å—
try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    from sklearn.feature_selection import SelectKBest, f_classif
    import joblib
    ML_AVAILABLE = True
except ImportError:
    print("âš ï¸  æœºå™¨å­¦ä¹ æ¨¡å—æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install scikit-learn joblib")
    ML_AVAILABLE = False

@dataclass
class MLPrediction:
    """MLé¢„æµ‹ç»“æœ"""
    direction: str              # 'up', 'down', 'sideways'
    confidence: float           # 0-1
    probability_up: float       # ä¸Šæ¶¨æ¦‚ç‡
    probability_down: float     # ä¸‹è·Œæ¦‚ç‡
    probability_sideways: float # éœ‡è¡æ¦‚ç‡
    feature_importance: Dict[str, float]
    
    def __str__(self):
        return f"é¢„æµ‹: {self.direction} (ç½®ä¿¡åº¦: {self.confidence:.3f})"

class MLTrendOptimizer:
    """æœºå™¨å­¦ä¹ è¶‹åŠ¿ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        if not ML_AVAILABLE:
            raise ImportError("æœºå™¨å­¦ä¹ æ¨¡å—æœªå®‰è£…")
        
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.label_encoders = {}
        
        # ç‰¹å¾å·¥ç¨‹å‚æ•°
        self.lookback_periods = [5, 10, 20, 50]
        self.prediction_horizon = 10  # é¢„æµ‹æœªæ¥10æœŸçš„è¶‹åŠ¿
        
        # æ¨¡å‹é…ç½®
        self.model_configs = {
            'random_forest': {
                'model': RandomForestClassifier,
                'params': {
                    'n_estimators': 100,
                    'max_depth': 15,
                    'min_samples_split': 5,
                    'min_samples_leaf': 2,
                    'random_state': 42
                }
            },
            'gradient_boosting': {
                'model': GradientBoostingClassifier,
                'params': {
                    'n_estimators': 100,
                    'learning_rate': 0.1,
                    'max_depth': 6,
                    'random_state': 42
                }
            },
            'logistic_regression': {
                'model': LogisticRegression,
                'params': {
                    'random_state': 42,
                    'max_iter': 1000
                }
            }
        }
        
    def extract_comprehensive_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """æå–ç»¼åˆç‰¹å¾"""
        
        features = pd.DataFrame(index=data.index)
        
        # åŸºç¡€ä»·æ ¼ç‰¹å¾
        features = self._add_price_features(features, data)
        
        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        features = self._add_technical_indicators(features, data)
        
        # ç»Ÿè®¡ç‰¹å¾
        features = self._add_statistical_features(features, data)
        
        # æ—¶é—´ç‰¹å¾
        features = self._add_temporal_features(features, data)
        
        # æˆäº¤é‡ç‰¹å¾
        features = self._add_volume_features(features, data)
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        features = self._add_volatility_features(features, data)
        
        # å½¢æ€è¯†åˆ«ç‰¹å¾
        features = self._add_pattern_features(features, data)
        
        return features.dropna()
    
    def _add_price_features(self, features: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ ä»·æ ¼ç›¸å…³ç‰¹å¾"""
        
        close = data['close']
        high = data['high']
        low = data['low']
        open_price = data['open']
        
        # åŸºç¡€ä»·æ ¼ç‰¹å¾
        features['price_change'] = close.pct_change()
        features['price_change_abs'] = features['price_change'].abs()
        features['high_low_ratio'] = high / low
        features['close_open_ratio'] = close / open_price
        features['upper_shadow'] = (high - np.maximum(close, open_price)) / (high - low + 1e-8)
        features['lower_shadow'] = (np.minimum(close, open_price) - low) / (high - low + 1e-8)
        features['body_ratio'] = np.abs(close - open_price) / (high - low + 1e-8)
        
        # å¤šå‘¨æœŸä»·æ ¼å˜åŒ–
        for period in [3, 5, 10, 20]:
            features[f'price_change_{period}d'] = close.pct_change(period)
            features[f'high_{period}d'] = high.rolling(period).max() / close
            features[f'low_{period}d'] = low.rolling(period).min() / close
        
        # ä»·æ ¼ä½ç½®ç‰¹å¾
        for period in [10, 20, 50]:
            rolling_high = high.rolling(period).max()
            rolling_low = low.rolling(period).min()
            features[f'price_position_{period}d'] = (close - rolling_low) / (rolling_high - rolling_low + 1e-8)
        
        return features
    
    def _add_technical_indicators(self, features: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
        
        close = data['close'].values
        high = data['high'].values
        low = data['low'].values
        volume = data.get('volume', pd.Series([1]*len(data))).values
        
        # è¶‹åŠ¿æŒ‡æ ‡
        for period in [10, 20, 50]:
            sma = talib.SMA(close, period)
            ema = talib.EMA(close, period)
            features[f'sma_{period}'] = close / sma
            features[f'ema_{period}'] = close / ema
            features[f'sma_slope_{period}'] = pd.Series(sma).pct_change(5)
        
        # åŠ¨é‡æŒ‡æ ‡
        features['rsi_14'] = talib.RSI(close, 14)
        features['rsi_7'] = talib.RSI(close, 7)
        features['rsi_21'] = talib.RSI(close, 21)
        
        # MACD
        macd, macd_signal, macd_hist = talib.MACD(close)
        features['macd'] = macd
        features['macd_signal'] = macd_signal
        features['macd_hist'] = macd_hist
        features['macd_ratio'] = macd / (macd_signal + 1e-8)
        
        # éšæœºæŒ‡æ ‡
        slowk, slowd = talib.STOCH(high, low, close)
        features['stoch_k'] = slowk
        features['stoch_d'] = slowd
        features['stoch_ratio'] = slowk / (slowd + 1e-8)
        
        # ADXç³»ç»Ÿ
        features['adx'] = talib.ADX(high, low, close, 14)
        features['plus_di'] = talib.PLUS_DI(high, low, close, 14)
        features['minus_di'] = talib.MINUS_DI(high, low, close, 14)
        features['di_ratio'] = features['plus_di'] / (features['minus_di'] + 1e-8)
        
        # å¨å»‰æŒ‡æ ‡
        features['williams_r'] = talib.WILLR(high, low, close, 14)
        
        # CCI
        features['cci'] = talib.CCI(high, low, close, 14)
        
        # å¸ƒæ—å¸¦
        bb_upper, bb_middle, bb_lower = talib.BBANDS(close)
        features['bb_upper'] = bb_upper
        features['bb_middle'] = bb_middle  
        features['bb_lower'] = bb_lower
        features['bb_width'] = (bb_upper - bb_lower) / bb_middle
        features['bb_position'] = (close - bb_lower) / (bb_upper - bb_lower + 1e-8)
        
        return features
    
    def _add_statistical_features(self, features: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ ç»Ÿè®¡ç‰¹å¾"""
        
        close = data['close']
        
        # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        for window in [5, 10, 20]:
            rolling_mean = close.rolling(window).mean()
            rolling_std = close.rolling(window).std()
            
            features[f'zscore_{window}'] = (close - rolling_mean) / (rolling_std + 1e-8)
            features[f'cv_{window}'] = rolling_std / (rolling_mean + 1e-8)
            features[f'skew_{window}'] = close.rolling(window).skew()
            features[f'kurt_{window}'] = close.rolling(window).kurt()
        
        # ç™¾åˆ†ä½æ•°ç‰¹å¾
        for window in [20, 50]:
            features[f'percentile_rank_{window}'] = close.rolling(window).rank(pct=True)
        
        return features
    
    def _add_temporal_features(self, features: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æ—¶é—´ç‰¹å¾"""
        
        if 'timestamp' in data.columns:
            timestamp = pd.to_datetime(data['timestamp'])
            features['hour'] = timestamp.dt.hour
            features['day_of_week'] = timestamp.dt.dayofweek
            features['day_of_month'] = timestamp.dt.day
            features['month'] = timestamp.dt.month
            
            # å‘¨æœŸæ€§ç‰¹å¾
            features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
            features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
            features['dow_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)
            features['dow_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)
        
        return features
    
    def _add_volume_features(self, features: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æˆäº¤é‡ç‰¹å¾"""
        
        if 'volume' not in data.columns:
            return features
        
        volume = data['volume']
        close = data['close']
        
        # æˆäº¤é‡æŒ‡æ ‡
        for period in [5, 10, 20]:
            vol_sma = volume.rolling(period).mean()
            features[f'volume_ratio_{period}'] = volume / (vol_sma + 1e-8)
            features[f'volume_price_trend_{period}'] = volume.rolling(period).corr(close)
        
        # OBV
        features['obv'] = talib.OBV(close.values, volume.values)
        features['obv_sma'] = talib.SMA(features['obv'].values, 10)
        features['obv_ratio'] = features['obv'] / (features['obv_sma'] + 1e-8)
        
        # æˆäº¤é‡ä»·æ ¼ç¡®è®¤
        price_change = close.pct_change()
        volume_change = volume.pct_change()
        features['vol_price_confirmation'] = (
            (price_change > 0) & (volume_change > 0) |
            (price_change < 0) & (volume_change > 0)
        ).astype(int)
        
        return features
    
    def _add_volatility_features(self, features: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æ³¢åŠ¨ç‡ç‰¹å¾"""
        
        high = data['high']
        low = data['low']
        close = data['close']
        
        # ATR
        for period in [7, 14, 21]:
            atr = talib.ATR(high.values, low.values, close.values, period)
            features[f'atr_{period}'] = atr
            features[f'atr_ratio_{period}'] = atr / close.values
        
        # çœŸå®æ³¢åŠ¨ç‡
        tr = talib.TRANGE(high.values, low.values, close.values)
        features['true_range'] = tr
        features['tr_ratio'] = tr / close.values
        
        # ä»·æ ¼å˜åŒ–çš„æ³¢åŠ¨ç‡
        returns = close.pct_change()
        for window in [5, 10, 20]:
            features[f'volatility_{window}'] = returns.rolling(window).std()
            features[f'volatility_ratio_{window}'] = (
                features[f'volatility_{window}'] / returns.rolling(50).std()
            )
        
        return features
    
    def _add_pattern_features(self, features: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ å½¢æ€è¯†åˆ«ç‰¹å¾"""
        
        open_price = data['open'].values
        high = data['high'].values
        low = data['low'].values
        close = data['close'].values
        
        # Kçº¿å½¢æ€
        features['doji'] = talib.CDLDOJI(open_price, high, low, close)
        features['hammer'] = talib.CDLHAMMER(open_price, high, low, close)
        features['shooting_star'] = talib.CDLSHOOTINGSTAR(open_price, high, low, close)
        features['engulfing_bullish'] = talib.CDLENGULFING(open_price, high, low, close)
        features['hanging_man'] = talib.CDLHANGINGMAN(open_price, high, low, close)
        features['inverted_hammer'] = talib.CDLINVERTEDHAMMER(open_price, high, low, close)
        
        # ä»·æ ¼çªç ´
        for period in [10, 20]:
            rolling_high = pd.Series(high).rolling(period).max()
            rolling_low = pd.Series(low).rolling(period).min()
            features[f'breakout_high_{period}'] = (close > rolling_high.shift(1)).astype(int)
            features[f'breakout_low_{period}'] = (close < rolling_low.shift(1)).astype(int)
        
        return features
    
    def generate_labels(self, data: pd.DataFrame, horizon: int = 10, 
                       threshold: float = 0.02) -> pd.Series:
        """ç”Ÿæˆè®­ç»ƒæ ‡ç­¾"""
        
        close = data['close']
        
        # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
        future_returns = close.shift(-horizon) / close - 1
        
        # ä¸‰åˆ†ç±»æ ‡ç­¾
        labels = pd.Series(index=data.index, dtype=str)
        labels[future_returns > threshold] = 'up'
        labels[future_returns < -threshold] = 'down'
        labels[(future_returns >= -threshold) & (future_returns <= threshold)] = 'sideways'
        
        # å»æ‰æ— æ³•é¢„æµ‹çš„æœ€åå‡ ä¸ªæ•°æ®ç‚¹
        labels = labels[:-horizon]
        
        return labels
    
    def train_ensemble_model(self, features: pd.DataFrame, labels: pd.Series,
                           test_size: float = 0.2) -> Dict[str, Any]:
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        
        print("ğŸ¯ å¼€å§‹è®­ç»ƒMLé›†æˆæ¨¡å‹...")
        
        # æ•°æ®é¢„å¤„ç†
        print("ğŸ”§ æ•°æ®é¢„å¤„ç†...")
        
        # ç§»é™¤æ— æ•ˆç‰¹å¾
        features = features.replace([np.inf, -np.inf], np.nan)
        features = features.fillna(method='ffill').fillna(0)
        
        # ç‰¹å¾é€‰æ‹©
        print("ğŸ“Š ç‰¹å¾é€‰æ‹©...")
        selector = SelectKBest(score_func=f_classif, k=min(50, features.shape[1]))
        features_selected = selector.fit_transform(features, labels)
        selected_feature_names = features.columns[selector.get_support()].tolist()
        
        print(f"   é€‰æ‹©äº† {len(selected_feature_names)} ä¸ªç‰¹å¾")
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            features_selected, labels, test_size=test_size, 
            random_state=42, stratify=labels
        )
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # æ ‡ç­¾ç¼–ç 
        le = LabelEncoder()
        y_train_encoded = le.fit_transform(y_train)
        y_test_encoded = le.transform(y_test)
        
        print(f"ğŸ“Š è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"ğŸ“Š æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y_train, return_counts=True)))}")
        
        # è®­ç»ƒå¤šä¸ªæ¨¡å‹
        trained_models = {}
        model_scores = {}
        
        for model_name, config in self.model_configs.items():
            print(f"\nğŸ”„ è®­ç»ƒ {model_name}...")
            
            # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
            model = config['model'](**config['params'])
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X_train_scaled, y_train_encoded, cv=5)
            print(f"   äº¤å‰éªŒè¯å¾—åˆ†: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
            
            # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            model.fit(X_train_scaled, y_train_encoded)
            
            # æµ‹è¯•é›†è¯„ä¼°
            test_score = model.score(X_test_scaled, y_test_encoded)
            print(f"   æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.3f}")
            
            trained_models[model_name] = model
            model_scores[model_name] = test_score
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model_name = max(model_scores, key=model_scores.get)
        best_model = trained_models[best_model_name]
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (å‡†ç¡®ç‡: {model_scores[best_model_name]:.3f})")
        
        # è¯¦ç»†è¯„ä¼°
        y_pred = best_model.predict(X_test_scaled)
        y_pred_labels = le.inverse_transform(y_pred)
        
        print(f"\nğŸ“Š è¯¦ç»†è¯„ä¼°æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred_labels))
        
        # ç‰¹å¾é‡è¦æ€§
        if hasattr(best_model, 'feature_importances_'):
            importance = best_model.feature_importances_
            feature_importance = dict(zip(selected_feature_names, importance))
            
            # æ’åºç‰¹å¾é‡è¦æ€§
            sorted_importance = sorted(feature_importance.items(), 
                                     key=lambda x: x[1], reverse=True)
            
            print(f"\nğŸ“ˆ å‰10ä¸ªé‡è¦ç‰¹å¾:")
            for i, (feature, imp) in enumerate(sorted_importance[:10]):
                print(f"   {i+1}. {feature}: {imp:.3f}")
        
        # ä¿å­˜æ¨¡å‹ç»„ä»¶
        model_package = {
            'model': best_model,
            'scaler': scaler,
            'selector': selector,
            'label_encoder': le,
            'feature_names': selected_feature_names,
            'model_name': best_model_name,
            'accuracy': model_scores[best_model_name],
            'feature_importance': feature_importance if hasattr(best_model, 'feature_importances_') else {}
        }
        
        return model_package
    
    def predict_trend(self, model_package: Dict[str, Any], 
                     features: pd.DataFrame) -> MLPrediction:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹è¶‹åŠ¿"""
        
        model = model_package['model']
        scaler = model_package['scaler']
        selector = model_package['selector']
        le = model_package['label_encoder']
        
        # é¢„å¤„ç†ç‰¹å¾
        features = features.replace([np.inf, -np.inf], np.nan)
        features = features.fillna(method='ffill').fillna(0)
        
        # é€‰æ‹©ç‰¹å¾
        features_selected = selector.transform(features.iloc[-1:])
        
        # æ ‡å‡†åŒ–
        features_scaled = scaler.transform(features_selected)
        
        # é¢„æµ‹
        prediction = model.predict(features_scaled)[0]
        probabilities = model.predict_proba(features_scaled)[0]
        
        # è§£ç æ ‡ç­¾
        direction = le.inverse_transform([prediction])[0]
        
        # è·å–å„ç±»åˆ«æ¦‚ç‡ - ä¿®å¤ç‰ˆ
        class_labels = le.classes_  # è¿™æ˜¯å­—ç¬¦ä¸²æ ‡ç­¾ ['down', 'sideways', 'up']
        prob_dict = dict(zip(class_labels, probabilities))  # ç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦inverse_transform
        
        # ç½®ä¿¡åº¦ï¼ˆæœ€é«˜æ¦‚ç‡ï¼‰
        confidence = max(probabilities)
        
        return MLPrediction(
            direction=direction,
            confidence=confidence,
            probability_up=prob_dict.get('up', 0),
            probability_down=prob_dict.get('down', 0),
            probability_sideways=prob_dict.get('sideways', 0),
            feature_importance=model_package.get('feature_importance', {})
        )
    
    def save_model(self, model_package: Dict[str, Any], filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_package, f)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str) -> Dict[str, Any]:
        """åŠ è½½æ¨¡å‹"""
        
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        with open(filepath, 'rb') as f:
            model_package = pickle.load(f)
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {filepath}")
        print(f"   æ¨¡å‹ç±»å‹: {model_package['model_name']}")
        print(f"   å‡†ç¡®ç‡: {model_package['accuracy']:.3f}")
        
        return model_package
    
    def optimize_parameters_with_ml(self, data: pd.DataFrame, 
                                   model_package: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨MLç»“æœä¼˜åŒ–ç­–ç•¥å‚æ•°"""
        
        # æå–ç‰¹å¾
        features = self.extract_comprehensive_features(data)
        
        # é¢„æµ‹è¶‹åŠ¿
        prediction = self.predict_trend(model_package, features)
        
        print(f"ğŸ¤– MLé¢„æµ‹ç»“æœ: {prediction}")
        
        # æ ¹æ®MLé¢„æµ‹è°ƒæ•´å‚æ•°
        base_params = {
            'min_shadow_body_ratio': 2.0,
            'max_body_ratio': 0.30,
            'min_signal_score': 3,
            'volume_threshold': 1.2,
            'adx_threshold': 20,
            'trend_profit_extension': False,
            'max_trend_profit_pct': 8.0
        }
        
        # æ ¹æ®MLç½®ä¿¡åº¦è°ƒæ•´
        if prediction.confidence > 0.8:
            base_params['min_signal_score'] = 2  # é«˜ç½®ä¿¡åº¦é™ä½è¦æ±‚
            base_params['trend_profit_extension'] = True
            
        if prediction.direction == 'up' and prediction.probability_up > 0.7:
            base_params['max_trend_profit_pct'] = 12.0  # çœ‹æ¶¨æ—¶æé«˜ç›®æ ‡
        elif prediction.direction == 'down' and prediction.probability_down > 0.7:
            base_params['max_trend_profit_pct'] = 12.0  # çœ‹è·Œæ—¶æé«˜ç›®æ ‡
        
        # æ ¹æ®é‡è¦ç‰¹å¾è°ƒæ•´
        important_features = prediction.feature_importance
        
        # å¦‚æœRSIé‡è¦æ€§é«˜ï¼Œè°ƒæ•´RSIç›¸å…³å‚æ•°
        rsi_importance = sum(v for k, v in important_features.items() if 'rsi' in k.lower())
        if rsi_importance > 0.1:
            base_params['rsi_weight'] = 1.5
        
        # å¦‚æœæˆäº¤é‡é‡è¦æ€§é«˜ï¼Œè°ƒæ•´æˆäº¤é‡ç›¸å…³å‚æ•°
        volume_importance = sum(v for k, v in important_features.items() if 'volume' in k.lower())
        if volume_importance > 0.1:
            base_params['volume_threshold'] = 1.0  # é™ä½æˆäº¤é‡è¦æ±‚
        
        return base_params
    
    def generate_ml_report(self, model_package: Dict[str, Any], 
                          prediction: MLPrediction) -> str:
        """ç”ŸæˆMLåˆ†ææŠ¥å‘Š"""
        
        report = f"""
ğŸ¤– æœºå™¨å­¦ä¹ è¶‹åŠ¿åˆ†ææŠ¥å‘Š
{'='*60}
æ¨¡å‹ä¿¡æ¯:
  - æ¨¡å‹ç±»å‹: {model_package['model_name']}
  - è®­ç»ƒå‡†ç¡®ç‡: {model_package['accuracy']:.3f}
  - ç‰¹å¾æ•°é‡: {len(model_package['feature_names'])}

ğŸ“Š é¢„æµ‹ç»“æœ:
{'='*60}
é¢„æµ‹æ–¹å‘: {prediction.direction.upper()}
ç½®ä¿¡åº¦: {prediction.confidence:.3f}

å„æ–¹å‘æ¦‚ç‡:
  - ä¸Šæ¶¨æ¦‚ç‡: {prediction.probability_up:.3f}
  - ä¸‹è·Œæ¦‚ç‡: {prediction.probability_down:.3f}
  - éœ‡è¡æ¦‚ç‡: {prediction.probability_sideways:.3f}

ğŸ“ˆ å…³é”®å½±å“å› å­ (å‰10ä¸ª):
{'='*60}
"""
        
        # æ˜¾ç¤ºé‡è¦ç‰¹å¾
        sorted_features = sorted(prediction.feature_importance.items(), 
                               key=lambda x: x[1], reverse=True)
        
        for i, (feature, importance) in enumerate(sorted_features[:10]):
            report += f"  {i+1}. {feature}: {importance:.3f}\n"
        
        # äº¤æ˜“å»ºè®®
        report += f"""
ğŸ’¡ MLå¢å¼ºäº¤æ˜“å»ºè®®:
{'='*60}
"""
        
        if prediction.confidence > 0.8:
            report += f"âœ… é«˜ç½®ä¿¡åº¦{prediction.direction}ä¿¡å·ï¼Œå»ºè®®ç§¯æäº¤æ˜“\n"
        elif prediction.confidence > 0.6:
            report += f"âš ï¸  ä¸­ç­‰ç½®ä¿¡åº¦{prediction.direction}ä¿¡å·ï¼Œè°¨æ…äº¤æ˜“\n"
        else:
            report += f"âŒ ä½ç½®ä¿¡åº¦ä¿¡å·ï¼Œå»ºè®®è§‚æœ›\n"
        
        if prediction.direction != 'sideways':
            main_prob = max(prediction.probability_up, prediction.probability_down)
            if main_prob > 0.7:
                report += "ğŸ“ˆ æ¦‚ç‡åˆ†å¸ƒæ˜¾ç¤ºå¼ºçƒˆè¶‹åŠ¿å€¾å‘\n"
            elif main_prob > 0.5:
                report += "ğŸ“Š æ¦‚ç‡åˆ†å¸ƒæ˜¾ç¤ºæ¸©å’Œè¶‹åŠ¿å€¾å‘\n"
        
        return report

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    if ML_AVAILABLE:
        print("ğŸ¤– æœºå™¨å­¦ä¹ è¶‹åŠ¿ä¼˜åŒ–ç³»ç»Ÿå·²å°±ç»ª")
        print("   æ”¯æŒéšæœºæ£®æ—ã€æ¢¯åº¦æå‡ã€é€»è¾‘å›å½’æ¨¡å‹")
        print("   æä¾›50+æŠ€æœ¯ç‰¹å¾çš„è‡ªåŠ¨æå–")
        print("   åŒ…å«ç‰¹å¾é€‰æ‹©å’Œæ¨¡å‹é›†æˆåŠŸèƒ½")
    else:
        print("âŒ æœºå™¨å­¦ä¹ æ¨¡å—æœªå®‰è£…ï¼Œè¯·å®‰è£…scikit-learn")