#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šé…ç½®ç®¡ç†å™¨ - æ”¯æŒåŠ è½½å’Œå¯¹æ¯”å¤šä¸ªé…ç½®æ–‡ä»¶
æ·»åŠ åˆ°config.pyæˆ–ä½œä¸ºç‹¬ç«‹æ¨¡å—ä½¿ç”¨
"""

import json
import os
import glob
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import inquirer

@dataclass
class ConfigResult:
    """é…ç½®å›æµ‹ç»“æœ"""
    config_name: str
    config_path: str
    strategy_name: str
    description: str
    risk_level: str
    backtest_result: Dict[str, Any]
    performance_score: float

class MultiConfigManager:
    """å¤šé…ç½®æ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = "configs"):
        self.config_dir = config_dir
        self.available_configs = {}
        self.loaded_configs = {}
        self.comparison_results = []
        
        # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
        os.makedirs(self.config_dir, exist_ok=True)
        
        # æ‰«æå¯ç”¨é…ç½®
        self.scan_available_configs()
    
    def scan_available_configs(self):
        """æ‰«æå¯ç”¨çš„é…ç½®æ–‡ä»¶"""
        # æ‰«æé…ç½®ç›®å½•ä¸­çš„JSONæ–‡ä»¶
        config_files = glob.glob(os.path.join(self.config_dir, "*.json"))
        
        # ä¹Ÿæ‰«ææ ¹ç›®å½•çš„é…ç½®æ–‡ä»¶
        root_configs = glob.glob("*_config.json")
        config_files.extend(root_configs)
        
        # æ·»åŠ é»˜è®¤é…ç½®
        if os.path.exists("strategy_config.json"):
            config_files.append("strategy_config.json")
        
        self.available_configs = {}
        
        for config_file in config_files:
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                
                # æå–é…ç½®ä¿¡æ¯
                config_name = os.path.basename(config_file).replace('.json', '')
                strategy_name = config_data.get('strategy_name', config_name)
                description = config_data.get('description', 'æ— æè¿°')
                risk_level = config_data.get('risk_level', 'æœªçŸ¥')
                
                self.available_configs[config_name] = {
                    'path': config_file,
                    'strategy_name': strategy_name,
                    'description': description,
                    'risk_level': risk_level,
                    'config_data': config_data
                }
                
                print(f"âœ… å‘ç°é…ç½®: {strategy_name}")
                
            except Exception as e:
                print(f"âŒ åŠ è½½é…ç½®å¤±è´¥ {config_file}: {e}")
        
        print(f"ğŸ“‹ æ€»å…±å‘ç° {len(self.available_configs)} ä¸ªé…ç½®æ–‡ä»¶")
    
    def list_available_configs(self) -> Dict[str, Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®"""
        return self.available_configs
    
    def load_config(self, config_name: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æŒ‡å®šé…ç½®"""
        if config_name in self.available_configs:
            config_info = self.available_configs[config_name]
            self.loaded_configs[config_name] = config_info['config_data']
            return config_info['config_data']
        else:
            print(f"âŒ é…ç½®ä¸å­˜åœ¨: {config_name}")
            return None
    
    def interactive_config_selection(self) -> List[str]:
        """äº¤äº’å¼é€‰æ‹©é…ç½®è¿›è¡Œå¯¹æ¯”"""
        if not self.available_configs:
            print("âŒ æ²¡æœ‰å‘ç°å¯ç”¨çš„é…ç½®æ–‡ä»¶")
            return []
        
        print("\nğŸ¯ å¯ç”¨çš„ç­–ç•¥é…ç½®:")
        print("=" * 60)
        
        # æ˜¾ç¤ºé…ç½®é€‰é¡¹
        config_choices = []
        for config_name, config_info in self.available_configs.items():
            strategy_name = config_info['strategy_name']
            description = config_info['description']
            risk_level = config_info['risk_level']
            
            choice_text = f"{strategy_name} ({risk_level}é£é™©) - {description}"
            config_choices.append((choice_text, config_name))
        
        # æ·»åŠ ç‰¹æ®Šé€‰é¡¹
        config_choices.extend([
            ("ğŸ”„ å…¨éƒ¨é…ç½®å¯¹æ¯”æµ‹è¯•", "all"),
            ("ğŸ“Š æ¨èé…ç½®ç»„åˆ", "recommended"),
            ("ğŸ¯ é«˜æ”¶ç›Šé…ç½®", "high_yield"),
            ("ğŸ›¡ï¸ ä½é£é™©é…ç½®", "low_risk")
        ])
        
        try:
            selected = inquirer.checkbox(
                "é€‰æ‹©è¦å¯¹æ¯”çš„é…ç½® (å¤šé€‰)",
                choices=config_choices
            )
            
            # å¤„ç†ç‰¹æ®Šé€‰é¡¹
            if "all" in selected:
                return list(self.available_configs.keys())
            elif "recommended" in selected:
                return self.get_recommended_configs()
            elif "high_yield" in selected:
                return self.get_high_yield_configs()
            elif "low_risk" in selected:
                return self.get_low_risk_configs()
            else:
                return selected
                
        except KeyboardInterrupt:
            print("\næ“ä½œå–æ¶ˆ")
            return []
    
    def get_recommended_configs(self) -> List[str]:
        """è·å–æ¨èé…ç½®ç»„åˆ"""
        recommended = []
        for config_name, config_info in self.available_configs.items():
            strategy_name = config_info['strategy_name']
            if any(keyword in strategy_name.lower() for keyword in 
                   ['æ¿€è¿›çŸ­çº¿', 'è¶‹åŠ¿è·Ÿè¸ª', 'æ”¹è¿›ä¿å®ˆ']):
                recommended.append(config_name)
        
        return recommended[:3]  # é™åˆ¶ä¸º3ä¸ª
    
    def get_high_yield_configs(self) -> List[str]:
        """è·å–é«˜æ”¶ç›Šé…ç½®"""
        high_yield = []
        for config_name, config_info in self.available_configs.items():
            strategy_name = config_info['strategy_name']
            if any(keyword in strategy_name.lower() for keyword in 
                   ['æ¿€è¿›', 'çªç ´', 'åŠ¨é‡']):
                high_yield.append(config_name)
        
        return high_yield
    
    def get_low_risk_configs(self) -> List[str]:
        """è·å–ä½é£é™©é…ç½®"""
        low_risk = []
        for config_name, config_info in self.available_configs.items():
            risk_level = config_info['risk_level']
            if risk_level in ['ä½', 'ä¸­']:
                low_risk.append(config_name)
        
        return low_risk
    
    def calculate_performance_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—æ€§èƒ½ç»¼åˆå¾—åˆ†"""
        try:
            total_return = result.get('total_return', 0)
            win_rate = result.get('win_rate', 0)
            max_drawdown = result.get('max_drawdown', 100)
            profit_factor = result.get('profit_factor', 0)
            sharpe_ratio = result.get('sharpe_ratio', 0)
            total_trades = result.get('total_trades', 0)
            
            # æƒé‡è®¾ç½®
            weights = {
                'return': 0.3,
                'win_rate': 0.2,
                'drawdown': 0.2,
                'profit_factor': 0.15,
                'sharpe': 0.1,
                'trades': 0.05
            }
            
            # æ ‡å‡†åŒ–æŒ‡æ ‡ (0-100åˆ†)
            return_score = min(100, max(0, total_return * 2))  # 50%æ”¶ç›Š=100åˆ†
            win_rate_score = min(100, win_rate * 1.5)  # 67%èƒœç‡=100åˆ†
            drawdown_score = max(0, 100 - max_drawdown * 4)  # 25%å›æ’¤=0åˆ†
            pf_score = min(100, profit_factor * 30)  # 3.33ç›ˆäºæ¯”=100åˆ†
            sharpe_score = min(100, max(0, sharpe_ratio * 40))  # 2.5å¤æ™®=100åˆ†
            trades_score = min(100, total_trades * 2)  # 50ç¬”äº¤æ˜“=100åˆ†
            
            # è®¡ç®—åŠ æƒå¾—åˆ†
            final_score = (
                return_score * weights['return'] +
                win_rate_score * weights['win_rate'] +
                drawdown_score * weights['drawdown'] +
                pf_score * weights['profit_factor'] +
                sharpe_score * weights['sharpe'] +
                trades_score * weights['trades']
            )
            
            return round(final_score, 2)
            
        except Exception as e:
            print(f"è®¡ç®—å¾—åˆ†å¤±è´¥: {e}")
            return 0.0
    
    def add_comparison_result(self, config_name: str, backtest_result: Dict[str, Any]):
        """æ·»åŠ å¯¹æ¯”ç»“æœ"""
        if config_name not in self.available_configs:
            print(f"è­¦å‘Š: é…ç½® {config_name} ä¸å­˜åœ¨")
            return
        
        config_info = self.available_configs[config_name]
        performance_score = self.calculate_performance_score(backtest_result)
        
        result = ConfigResult(
            config_name=config_name,
            config_path=config_info['path'],
            strategy_name=config_info['strategy_name'],
            description=config_info['description'],
            risk_level=config_info['risk_level'],
            backtest_result=backtest_result,
            performance_score=performance_score
        )
        
        self.comparison_results.append(result)
    
    def get_comparison_summary(self) -> Dict[str, Any]:
        """è·å–å¯¹æ¯”æ±‡æ€»"""
        if not self.comparison_results:
            return {}
        
        # æŒ‰å¾—åˆ†æ’åº
        sorted_results = sorted(self.comparison_results, 
                              key=lambda x: x.performance_score, reverse=True)
        
        # æ‰¾å‡ºå„é¡¹æœ€ä½³
        best_return = max(self.comparison_results, 
                         key=lambda x: x.backtest_result.get('total_return', 0))
        best_winrate = max(self.comparison_results, 
                          key=lambda x: x.backtest_result.get('win_rate', 0))
        best_risk = min(self.comparison_results, 
                       key=lambda x: x.backtest_result.get('max_drawdown', 100))
        best_sharpe = max(self.comparison_results, 
                         key=lambda x: x.backtest_result.get('sharpe_ratio', 0))
        
        return {
            'total_configs': len(self.comparison_results),
            'best_overall': sorted_results[0],
            'worst_overall': sorted_results[-1],
            'best_return': best_return,
            'best_winrate': best_winrate,
            'best_risk': best_risk,
            'best_sharpe': best_sharpe,
            'all_results': sorted_results,
            'average_return': sum(r.backtest_result.get('total_return', 0) 
                                for r in self.comparison_results) / len(self.comparison_results),
            'average_score': sum(r.performance_score for r in self.comparison_results) / len(self.comparison_results)
        }
    
    def print_comparison_summary(self):
        """æ‰“å°å¯¹æ¯”æ±‡æ€»"""
        summary = self.get_comparison_summary()
        
        if not summary:
            print("âŒ æ²¡æœ‰å¯¹æ¯”ç»“æœ")
            return
        
        print("\n" + "=" * 80)
        print("ğŸ“Š å¤šé…ç½®å¯¹æ¯”æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 80)
        
        print(f"ğŸ”¢ æµ‹è¯•é…ç½®æ•°é‡: {summary['total_configs']}")
        print(f"ğŸ“ˆ å¹³å‡æ”¶ç›Šç‡: {summary['average_return']:.2f}%")
        print(f"ğŸ† å¹³å‡ç»¼åˆå¾—åˆ†: {summary['average_score']:.1f}/100")
        
        print(f"\nğŸ¥‡ æœ€ä½³ç»¼åˆè¡¨ç°:")
        best = summary['best_overall']
        result = best.backtest_result
        print(f"   ç­–ç•¥: {best.strategy_name}")
        print(f"   æ”¶ç›Šç‡: {result.get('total_return', 0):.2f}%")
        print(f"   èƒœç‡: {result.get('win_rate', 0):.1f}%")
        print(f"   æœ€å¤§å›æ’¤: {result.get('max_drawdown', 0):.2f}%")
        print(f"   ç»¼åˆå¾—åˆ†: {best.performance_score:.1f}/100")
        
        print(f"\nğŸ¯ å„é¡¹æœ€ä½³è¡¨ç°:")
        print(f"   ğŸ’° æœ€é«˜æ”¶ç›Š: {summary['best_return'].strategy_name} "
              f"({summary['best_return'].backtest_result.get('total_return', 0):.2f}%)")
        print(f"   ğŸ¯ æœ€é«˜èƒœç‡: {summary['best_winrate'].strategy_name} "
              f"({summary['best_winrate'].backtest_result.get('win_rate', 0):.1f}%)")
        print(f"   ğŸ›¡ï¸  æœ€ä½å›æ’¤: {summary['best_risk'].strategy_name} "
              f"({summary['best_risk'].backtest_result.get('max_drawdown', 0):.2f}%)")
        print(f"   ğŸ“Š æœ€é«˜å¤æ™®: {summary['best_sharpe'].strategy_name} "
              f"({summary['best_sharpe'].backtest_result.get('sharpe_ratio', 0):.3f})")
        
        print(f"\nğŸ“‹ è¯¦ç»†æ’å:")
        for i, result in enumerate(summary['all_results'], 1):
            backtest = result.backtest_result
            print(f"   {i:2d}. {result.strategy_name:<20} "
                  f"æ”¶ç›Š:{backtest.get('total_return', 0):6.2f}% "
                  f"èƒœç‡:{backtest.get('win_rate', 0):5.1f}% "
                  f"å›æ’¤:{backtest.get('max_drawdown', 0):5.2f}% "
                  f"å¾—åˆ†:{result.performance_score:5.1f}")
    
    def export_comparison_results(self, filename: str = None):
        """å¯¼å‡ºå¯¹æ¯”ç»“æœ"""
        if not self.comparison_results:
            print("âŒ æ²¡æœ‰å¯¹æ¯”ç»“æœå¯å¯¼å‡º")
            return
        
        if filename is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"config_comparison_{timestamp}.json"
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_data = {
            'comparison_time': str(datetime.now()),
            'total_configs': len(self.comparison_results),
            'summary': self.get_comparison_summary(),
            'detailed_results': []
        }
        
        for result in self.comparison_results:
            export_data['detailed_results'].append({
                'config_name': result.config_name,
                'config_path': result.config_path,
                'strategy_name': result.strategy_name,
                'description': result.description,
                'risk_level': result.risk_level,
                'performance_score': result.performance_score,
                'backtest_result': result.backtest_result
            })
        
        # ä¿å­˜æ–‡ä»¶
        os.makedirs('comparison_results', exist_ok=True)
        filepath = os.path.join('comparison_results', filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"âœ… å¯¹æ¯”ç»“æœå·²å¯¼å‡ºåˆ°: {filepath}")
        return filepath

# åœ¨config.pyä¸­æ·»åŠ çš„æ–¹æ³•
def enhanced_config_manager_methods():
    """åœ¨ConfigManagerç±»ä¸­æ·»åŠ çš„æ–¹æ³•"""
    
    def load_multiple_configs_for_comparison(self, config_names: List[str] = None) -> List[Dict[str, Any]]:
        """åŠ è½½å¤šä¸ªé…ç½®ç”¨äºå¯¹æ¯”"""
        multi_manager = MultiConfigManager()
        
        if config_names is None:
            config_names = multi_manager.interactive_config_selection()
        
        if not config_names:
            return []
        
        loaded_configs = []
        for config_name in config_names:
            config_data = multi_manager.load_config(config_name)
            if config_data:
                config_data['_config_name'] = config_name
                config_data['_strategy_name'] = multi_manager.available_configs[config_name]['strategy_name']
                loaded_configs.append(config_data)
        
        return loaded_configs
    
    def run_multi_config_comparison(self, data_manager, strategy_runner):
        """è¿è¡Œå¤šé…ç½®å¯¹æ¯”æµ‹è¯•"""
        multi_manager = MultiConfigManager()
        
        # é€‰æ‹©é…ç½®
        config_names = multi_manager.interactive_config_selection()
        if not config_names:
            return None
        
        print(f"\nğŸš€ å¼€å§‹å¯¹æ¯”æµ‹è¯• {len(config_names)} ä¸ªé…ç½®...")
        
        # é€ä¸ªè¿è¡Œå›æµ‹
        for i, config_name in enumerate(config_names, 1):
            print(f"\nğŸ“Š æµ‹è¯•é…ç½® {i}/{len(config_names)}: {config_name}")
            
            # åŠ è½½é…ç½®
            config_data = multi_manager.load_config(config_name)
            if not config_data:
                continue
            
            try:
                # åº”ç”¨é…ç½®åˆ°å½“å‰ç®¡ç†å™¨
                self.apply_config_data(config_data)
                
                # è·å–æ•°æ®
                symbol = self.backtest_params.symbol
                data = data_manager.get_historical_data(
                    symbol=symbol,
                    interval=self.backtest_params.interval,
                    start_date=self.backtest_params.start_date,
                    end_date=self.backtest_params.end_date
                )
                
                if data is None:
                    print(f"âŒ {config_name} æ•°æ®è·å–å¤±è´¥")
                    continue
                
                # è¿è¡Œç­–ç•¥
                result = strategy_runner(data, self.trading_params, self.backtest_params)
                
                # æ·»åŠ ç»“æœ
                multi_manager.add_comparison_result(config_name, result)
                
                print(f"âœ… {config_name} æµ‹è¯•å®Œæˆ - æ”¶ç›Šç‡: {result.get('total_return', 0):.2f}%")
                
            except Exception as e:
                print(f"âŒ {config_name} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
        multi_manager.print_comparison_summary()
        
        # å¯¼å‡ºç»“æœ
        if inquirer.confirm("æ˜¯å¦å¯¼å‡ºå¯¹æ¯”ç»“æœ?", default=True):
            multi_manager.export_comparison_results()
        
        return multi_manager
    
    def apply_config_data(self, config_data: Dict[str, Any]):
        """åº”ç”¨é…ç½®æ•°æ®åˆ°å½“å‰ç®¡ç†å™¨"""
        # åº”ç”¨äº¤æ˜“å‚æ•°
        if 'trading_params' in config_data:
            for key, value in config_data['trading_params'].items():
                if hasattr(self.trading_params, key):
                    setattr(self.trading_params, key, value)
        
        # åº”ç”¨å›æµ‹å‚æ•°
        if 'backtest_params' in config_data:
            for key, value in config_data['backtest_params'].items():
                if hasattr(self.backtest_params, key):
                    setattr(self.backtest_params, key, value)
        
        # åº”ç”¨ä¿¡å·é…ç½®
        if 'signal_config' in config_data:
            self.signal_config = config_data['signal_config']

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•å¤šé…ç½®ç®¡ç†å™¨
    manager = MultiConfigManager()
    
    # åˆ—å‡ºå¯ç”¨é…ç½®
    configs = manager.list_available_configs()
    print(f"å‘ç° {len(configs)} ä¸ªé…ç½®")
    
    # äº¤äº’å¼é€‰æ‹©
    selected = manager.interactive_config_selection()
    print(f"é€‰æ‹©äº†: {selected}")
    
    # æ¨¡æ‹Ÿæ·»åŠ ç»“æœ
    for config_name in selected[:2]:
        # æ¨¡æ‹Ÿå›æµ‹ç»“æœ
        mock_result = {
            'total_return': 5.5,
            'win_rate': 65.0,
            'max_drawdown': 12.0,
            'profit_factor': 1.8,
            'sharpe_ratio': 1.2,
            'total_trades': 25
        }
        manager.add_comparison_result(config_name, mock_result)
    
    # æ˜¾ç¤ºæ±‡æ€»
    manager.print_comparison_summary()

