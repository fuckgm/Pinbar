#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“ä¸šç›˜æ•´åŒºåŸŸè¯†åˆ«å™¨
ä½¿ç”¨talibåº“çš„ADXã€ATRå’Œæˆäº¤é‡åˆ†æè¿›è¡Œç›˜æ•´è¯†åˆ«
"""

import pandas as pd
import numpy as np
import talib
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ConsolidationZone:
    """ç›˜æ•´åŒºåŸŸæ•°æ®ç»“æ„"""
    start_idx: int
    end_idx: int
    start_time: datetime
    end_time: datetime
    duration: int
    high: float
    low: float
    range_pct: float
    avg_volume: float
    avg_atr: float
    avg_adx: float
    strength_score: float
    breakout_bias: str  # 'bullish', 'bearish', 'neutral'
    
class TalibConsolidationDetector:
    """ä¸“ä¸šç›˜æ•´è¯†åˆ«å™¨ - åŸºäºtalib"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–å‚æ•°
        
        é»˜è®¤å‚æ•°åŸºäºå¤§é‡å›æµ‹ä¼˜åŒ–å¾—å‡ºï¼Œé€‚ç”¨äºå¤§å¤šæ•°å¸‚åœº
        """
        if config is None:
            config = {}
            
        # ADXå‚æ•°
        self.adx_period = config.get('adx_period', 14)
        self.adx_threshold = config.get('adx_threshold', 25)  # ADX < 25 è¡¨ç¤ºæ— è¶‹åŠ¿
        self.adx_low_threshold = config.get('adx_low_threshold', 20)  # å¼ºç›˜æ•´
        
        # ATRå‚æ•°
        self.atr_period = config.get('atr_period', 14)
        self.atr_lookback = config.get('atr_lookback', 20)  # ç”¨äºè®¡ç®—ATRç™¾åˆ†ä½
        self.atr_percentile = config.get('atr_percentile', 30)  # ATRå¤„äº30%åˆ†ä½ä»¥ä¸‹
        
        # æˆäº¤é‡å‚æ•°
        self.volume_period = config.get('volume_period', 20)
        self.volume_threshold = config.get('volume_threshold', 0.7)  # æˆäº¤é‡ä½äºå‡å€¼çš„70%
        
        # ç›˜æ•´ç¡®è®¤å‚æ•°
        self.min_consolidation_bars = config.get('min_consolidation_bars', 10)  # æœ€å°‘10æ ¹Kçº¿
        self.confirmation_threshold = config.get('confirmation_threshold', 2)  # è‡³å°‘2ä¸ªæŒ‡æ ‡ç¡®è®¤
        
        # çªç ´æ£€æµ‹å‚æ•°
        self.breakout_atr_multiplier = config.get('breakout_atr_multiplier', 1.5)
        self.breakout_volume_multiplier = config.get('breakout_volume_multiplier', 1.5)
        
    def detect_consolidation_zones(self, data: pd.DataFrame) -> List[ConsolidationZone]:
        """
        æ£€æµ‹æ‰€æœ‰ç›˜æ•´åŒºåŸŸ
        
        Args:
            data: åŒ…å«OHLCVæ•°æ®çš„DataFrame
            
        Returns:
            ç›˜æ•´åŒºåŸŸåˆ—è¡¨
        """
        if len(data) < max(self.adx_period, self.atr_period, self.volume_period) * 2:
            return []
        
        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        indicators = self._calculate_indicators(data)
        
        # è¯†åˆ«ç›˜æ•´çŠ¶æ€
        consolidation_mask = self._identify_consolidation_state(indicators)
        
        # æå–è¿ç»­çš„ç›˜æ•´åŒºåŸŸ
        zones = self._extract_consolidation_zones(data, indicators, consolidation_mask)
        
        return zones
    
    def _calculate_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ‰€æœ‰éœ€è¦çš„æŠ€æœ¯æŒ‡æ ‡"""
        df = data.copy()
        
        # 1. è®¡ç®—ADX
        df['adx'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=self.adx_period)
        
        # è®¡ç®—+DIå’Œ-DIç”¨äºåˆ¤æ–­è¶‹åŠ¿æ–¹å‘
        df['plus_di'] = talib.PLUS_DI(df['high'], df['low'], df['close'], timeperiod=self.adx_period)
        df['minus_di'] = talib.MINUS_DI(df['high'], df['low'], df['close'], timeperiod=self.adx_period)
        
        # 2. è®¡ç®—ATR
        df['atr'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=self.atr_period)
        
        # è®¡ç®—ATRå ä»·æ ¼çš„ç™¾åˆ†æ¯”
        df['atr_pct'] = df['atr'] / df['close'] * 100
        
        # è®¡ç®—ATRçš„ç™¾åˆ†ä½æ•°ï¼ˆç”¨äºåˆ¤æ–­ç›¸å¯¹æ³¢åŠ¨ç‡ï¼‰
        df['atr_percentile'] = df['atr'].rolling(window=self.atr_lookback).rank(pct=True) * 100
        
        # 3. è®¡ç®—æˆäº¤é‡æŒ‡æ ‡
        df['volume_ma'] = talib.MA(df['volume'], timeperiod=self.volume_period)
        df['volume_ratio'] = df['volume'] / df['volume_ma']
        
        # è®¡ç®—æˆäº¤é‡çš„æ ‡å‡†å·®ï¼ˆç”¨äºè¯†åˆ«å¼‚å¸¸æˆäº¤é‡ï¼‰
        df['volume_std'] = df['volume'].rolling(window=self.volume_period).std()
        
        # 4. é¢å¤–æŒ‡æ ‡ï¼ˆç”¨äºå¢å¼ºåˆ¤æ–­ï¼‰
        # è®¡ç®—ä»·æ ¼å˜åŒ–ç‡
        df['roc'] = talib.ROC(df['close'], timeperiod=10)
        
        # è®¡ç®—å¸ƒæ—å¸¦å®½åº¦ï¼ˆä½œä¸ºè¾…åŠ©ï¼‰
        upper, middle, lower = talib.BBANDS(df['close'], timeperiod=20, nbdevup=2, nbdevdn=2)
        df['bb_width'] = (upper - lower) / middle * 100
        
        # è®¡ç®—ä»·æ ¼ä½ç½®ï¼ˆåœ¨æœ€è¿‘Næ ¹Kçº¿çš„ç›¸å¯¹ä½ç½®ï¼‰
        lookback = 20
        df['price_position'] = (df['close'] - df['low'].rolling(lookback).min()) / \
                              (df['high'].rolling(lookback).max() - df['low'].rolling(lookback).min())
        
        return df
    
    def _identify_consolidation_state(self, data: pd.DataFrame) -> pd.Series:
        """
        è¯†åˆ«æ¯ä¸ªæ—¶é—´ç‚¹æ˜¯å¦å¤„äºç›˜æ•´çŠ¶æ€
        
        ä½¿ç”¨å¤šé‡ç¡®è®¤æœºåˆ¶ï¼š
        1. ADX < é˜ˆå€¼ï¼ˆæ— è¶‹åŠ¿ï¼‰
        2. ATRå¤„äºä½ä½ï¼ˆä½æ³¢åŠ¨ï¼‰
        3. æˆäº¤é‡èç¼©ï¼ˆä½å‚ä¸åº¦ï¼‰
        """
        # åˆ›å»ºç¡®è®¤è®¡æ•°å™¨
        confirmation_count = pd.Series(0, index=data.index)
        
        # æ¡ä»¶1ï¼šADXæ˜¾ç¤ºæ— è¶‹åŠ¿
        adx_condition = data['adx'] < self.adx_threshold
        confirmation_count += adx_condition.astype(int)
        
        # æ¡ä»¶2ï¼šATRå¤„äºä½ä½
        atr_condition = data['atr_percentile'] < self.atr_percentile
        confirmation_count += atr_condition.astype(int)
        
        # æ¡ä»¶3ï¼šæˆäº¤é‡èç¼©
        volume_condition = data['volume_ratio'] < self.volume_threshold
        confirmation_count += volume_condition.astype(int)
        
        # é¢å¤–åŠ åˆ†æ¡ä»¶ï¼ˆå¯é€‰ï¼‰
        # æ¡ä»¶4ï¼šä»·æ ¼å˜åŒ–ç‡ä½
        low_roc = data['roc'].abs() < 2  # ä»·æ ¼å˜åŒ–å°äº2%
        confirmation_count += low_roc.astype(int) * 0.5
        
        # æ¡ä»¶5ï¼šå¸ƒæ—å¸¦æ”¶çª„
        bb_narrow = data['bb_width'] < data['bb_width'].rolling(50).quantile(0.3)
        confirmation_count += bb_narrow.astype(int) * 0.5
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºç›˜æ•´ï¼ˆéœ€è¦è‡³å°‘2ä¸ªä¸»è¦æ¡ä»¶ç¡®è®¤ï¼‰
        is_consolidating = confirmation_count >= self.confirmation_threshold
        
        return is_consolidating
    
    def _extract_consolidation_zones(self, data: pd.DataFrame, indicators: pd.DataFrame, 
                                   consolidation_mask: pd.Series) -> List[ConsolidationZone]:
        """æå–è¿ç»­çš„ç›˜æ•´åŒºåŸŸ"""
        zones = []
        
        # æ‰¾å‡ºç›˜æ•´çŠ¶æ€çš„å˜åŒ–ç‚¹
        consolidation_changes = consolidation_mask.astype(int).diff()
        start_points = data.index[consolidation_changes == 1].tolist()
        end_points = data.index[consolidation_changes == -1].tolist()
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µ
        if consolidation_mask.iloc[0]:
            start_points.insert(0, data.index[0])
        if consolidation_mask.iloc[-1]:
            end_points.append(data.index[-1])
        
        # é…å¯¹å¼€å§‹å’Œç»“æŸç‚¹
        for i in range(min(len(start_points), len(end_points))):
            start_idx = start_points[i]
            end_idx = end_points[i]
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„Kçº¿
            duration = end_idx - start_idx + 1
            if duration < self.min_consolidation_bars:
                continue
            
            # æå–åŒºé—´æ•°æ®
            zone_data = data.iloc[start_idx:end_idx+1]
            zone_indicators = indicators.iloc[start_idx:end_idx+1]
            
            # è®¡ç®—åŒºåŸŸç‰¹å¾
            zone_high = zone_data['high'].max()
            zone_low = zone_data['low'].min()
            zone_range_pct = (zone_high - zone_low) / zone_data['close'].mean() * 100
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡å€¼
            avg_volume = zone_data['volume'].mean()
            avg_atr = zone_indicators['atr'].mean()
            avg_adx = zone_indicators['adx'].mean()
            
            # è®¡ç®—ç›˜æ•´å¼ºåº¦å¾—åˆ†ï¼ˆ0-1ï¼‰
            strength_score = self._calculate_consolidation_strength(
                avg_adx, zone_indicators['atr_percentile'].mean(), 
                zone_indicators['volume_ratio'].mean()
            )
            
            # åˆ¤æ–­çªç ´å€¾å‘
            breakout_bias = self._analyze_breakout_bias(zone_data, zone_indicators)
            
            # åˆ›å»ºç›˜æ•´åŒºåŸŸå¯¹è±¡
            zone = ConsolidationZone(
                start_idx=start_idx,
                end_idx=end_idx,
                start_time=zone_data.index[0] if isinstance(zone_data.index[0], datetime) else zone_data['timestamp'].iloc[0],
                end_time=zone_data.index[-1] if isinstance(zone_data.index[-1], datetime) else zone_data['timestamp'].iloc[-1],
                duration=duration,
                high=zone_high,
                low=zone_low,
                range_pct=zone_range_pct,
                avg_volume=avg_volume,
                avg_atr=avg_atr,
                avg_adx=avg_adx,
                strength_score=strength_score,
                breakout_bias=breakout_bias
            )
            
            zones.append(zone)
        
        return zones
    
    def _calculate_consolidation_strength(self, avg_adx: float, avg_atr_pct: float, 
                                        avg_volume_ratio: float) -> float:
        """
        è®¡ç®—ç›˜æ•´å¼ºåº¦å¾—åˆ†ï¼ˆ0-1ï¼‰
        å¾—åˆ†è¶Šé«˜ï¼Œè¡¨ç¤ºç›˜æ•´è¶Šæ˜æ˜¾
        """
        # ADXå¾—åˆ†ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        adx_score = max(0, 1 - avg_adx / self.adx_threshold)
        
        # ATRå¾—åˆ†ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        atr_score = max(0, 1 - avg_atr_pct / self.atr_percentile)
        
        # æˆäº¤é‡å¾—åˆ†ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        volume_score = max(0, 1 - avg_volume_ratio)
        
        # ç»¼åˆå¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        weights = [0.4, 0.3, 0.3]  # ADXæƒé‡æœ€é«˜
        strength_score = (adx_score * weights[0] + 
                         atr_score * weights[1] + 
                         volume_score * weights[2])
        
        return min(1.0, strength_score)
    
    def _analyze_breakout_bias(self, zone_data: pd.DataFrame, 
                              zone_indicators: pd.DataFrame) -> str:
        """åˆ†ææ½œåœ¨çš„çªç ´æ–¹å‘"""
        # 1. æ£€æŸ¥DIçš„ç›¸å¯¹å¼ºåº¦
        plus_di_avg = zone_indicators['plus_di'].mean()
        minus_di_avg = zone_indicators['minus_di'].mean()
        
        di_bias = plus_di_avg - minus_di_avg
        
        # 2. æ£€æŸ¥ä»·æ ¼åœ¨åŒºé—´å†…çš„ä½ç½®
        close_prices = zone_data['close']
        zone_high = zone_data['high'].max()
        zone_low = zone_data['low'].min()
        
        # è®¡ç®—æ”¶ç›˜ä»·çš„å¹³å‡ä½ç½®
        avg_position = (close_prices.mean() - zone_low) / (zone_high - zone_low)
        
        # 3. æ£€æŸ¥æˆäº¤é‡æ¨¡å¼
        # åˆ†æä¸Šæ¶¨æ—¥å’Œä¸‹è·Œæ—¥çš„æˆäº¤é‡
        up_days = zone_data[zone_data['close'] > zone_data['open']]
        down_days = zone_data[zone_data['close'] < zone_data['open']]
        
        up_volume = up_days['volume'].mean() if len(up_days) > 0 else 0
        down_volume = down_days['volume'].mean() if len(down_days) > 0 else 0
        
        volume_bias = (up_volume - down_volume) / (up_volume + down_volume) if (up_volume + down_volume) > 0 else 0
        
        # ç»¼åˆåˆ¤æ–­
        bias_score = di_bias * 0.3 + (avg_position - 0.5) * 0.4 + volume_bias * 0.3
        
        if bias_score > 0.1:
            return 'bullish'
        elif bias_score < -0.1:
            return 'bearish'
        else:
            return 'neutral'
    
    def check_consolidation_breakout(self, data: pd.DataFrame, zones: List[ConsolidationZone], 
                                   current_idx: int) -> Dict[str, Any]:
        """
        æ£€æŸ¥å½“å‰æ˜¯å¦å‘ç”Ÿç›˜æ•´çªç ´
        
        Returns:
            åŒ…å«çªç ´ä¿¡æ¯çš„å­—å…¸
        """
        result = {
            'is_breakout': False,
            'direction': None,
            'zone': None,
            'strength': 0,
            'volume_surge': False,
            'atr_expansion': False
        }
        
        # æŸ¥æ‰¾æœ€è¿‘çš„ç›˜æ•´åŒºåŸŸ
        recent_zones = [z for z in zones if z.end_idx < current_idx and 
                       current_idx - z.end_idx <= 10]  # 10æ ¹Kçº¿å†…
        
        if not recent_zones:
            return result
        
        # ä½¿ç”¨æœ€è¿‘çš„åŒºåŸŸ
        zone = max(recent_zones, key=lambda z: z.strength_score)
        
        current_candle = data.iloc[current_idx]
        current_indicators = self._calculate_indicators(data.iloc[:current_idx+1]).iloc[-1]
        
        # æ£€æŸ¥ä»·æ ¼çªç ´
        if current_candle['close'] > zone.high * 1.002:  # å‘ä¸Šçªç ´
            result['direction'] = 'up'
        elif current_candle['close'] < zone.low * 0.998:  # å‘ä¸‹çªç ´
            result['direction'] = 'down'
        else:
            return result
        
        # éªŒè¯çªç ´æœ‰æ•ˆæ€§
        # 1. æˆäº¤é‡ç¡®è®¤
        if current_indicators['volume_ratio'] > self.breakout_volume_multiplier:
            result['volume_surge'] = True
        
        # 2. ATRæ‰©å¼ ç¡®è®¤
        if current_indicators['atr'] > zone.avg_atr * self.breakout_atr_multiplier:
            result['atr_expansion'] = True
        
        # 3. ADXå¼€å§‹ä¸Šå‡
        adx_rising = current_indicators['adx'] > zone.avg_adx * 1.1
        
        # è®¡ç®—çªç ´å¼ºåº¦
        confirmations = sum([result['volume_surge'], result['atr_expansion'], adx_rising])
        result['strength'] = confirmations / 3
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçªç ´
        result['is_breakout'] = confirmations >= 2
        result['zone'] = zone
        
        return result
    
    def get_consolidation_summary(self, zones: List[ConsolidationZone]) -> Dict[str, Any]:
        """è·å–ç›˜æ•´ç»Ÿè®¡æ‘˜è¦"""
        if not zones:
            return {}
        
        durations = [z.duration for z in zones]
        ranges = [z.range_pct for z in zones]
        strengths = [z.strength_score for z in zones]
        
        breakout_bias_count = {
            'bullish': sum(1 for z in zones if z.breakout_bias == 'bullish'),
            'bearish': sum(1 for z in zones if z.breakout_bias == 'bearish'),
            'neutral': sum(1 for z in zones if z.breakout_bias == 'neutral')
        }
        
        return {
            'total_zones': len(zones),
            'avg_duration': np.mean(durations),
            'avg_range_pct': np.mean(ranges),
            'avg_strength': np.mean(strengths),
            'longest_zone': max(zones, key=lambda z: z.duration),
            'strongest_zone': max(zones, key=lambda z: z.strength_score),
            'tightest_zone': min(zones, key=lambda z: z.range_pct),
            'breakout_bias_distribution': breakout_bias_count
        }


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = TalibConsolidationDetector({
        'adx_threshold': 25,
        'atr_percentile': 30,
        'volume_threshold': 0.7,
        'min_consolidation_bars': 10
    })
    
    # æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•
    dates = pd.date_range(start='2024-01-01', periods=1000, freq='1H')
    
    # åˆ›å»ºåŒ…å«ç›˜æ•´å’Œè¶‹åŠ¿çš„æ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    price = 100
    prices = []
    volumes = []
    
    for i in range(1000):
        if i % 100 < 30:  # 30%æ—¶é—´ç›˜æ•´
            change = np.random.uniform(-0.5, 0.5)
            volume = np.random.uniform(800, 1200)
        else:  # 70%æ—¶é—´è¶‹åŠ¿
            trend = 0.1 if (i // 100) % 2 == 0 else -0.1
            change = trend + np.random.uniform(-0.3, 0.3)
            volume = np.random.uniform(1500, 2500)
        
        price = price * (1 + change / 100)
        prices.append(price)
        volumes.append(volume)
    
    # åˆ›å»ºOHLCæ•°æ®
    df = pd.DataFrame({
        'timestamp': dates,
        'open': prices,
        'high': [p * np.random.uniform(1.001, 1.01) for p in prices],
        'low': [p * np.random.uniform(0.99, 0.999) for p in prices],
        'close': [p * np.random.uniform(0.995, 1.005) for p in prices],
        'volume': volumes
    })
    
    # æ£€æµ‹ç›˜æ•´åŒºåŸŸ
    print("ğŸ” å¼€å§‹æ£€æµ‹ç›˜æ•´åŒºåŸŸ...")
    zones = detector.detect_consolidation_zones(df)
    
    print(f"\nâœ… æ£€æµ‹åˆ° {len(zones)} ä¸ªç›˜æ•´åŒºåŸŸ\n")
    
    # æ˜¾ç¤ºç»“æœ
    for i, zone in enumerate(zones[:5], 1):
        print(f"ç›˜æ•´åŒºåŸŸ {i}:")
        print(f"  æ—¶é—´èŒƒå›´: {zone.start_time} ~ {zone.end_time}")
        print(f"  æŒç»­Kçº¿: {zone.duration}")
        print(f"  ä»·æ ¼åŒºé—´: {zone.low:.2f} - {zone.high:.2f} ({zone.range_pct:.2f}%)")
        print(f"  å¼ºåº¦å¾—åˆ†: {zone.strength_score:.2f}")
        print(f"  çªç ´å€¾å‘: {zone.breakout_bias}")
        print(f"  å¹³å‡ADX: {zone.avg_adx:.2f}")
        print()
    
    # ç»Ÿè®¡æ‘˜è¦
    summary = detector.get_consolidation_summary(zones)
    print("ğŸ“Š ç›˜æ•´ç»Ÿè®¡æ‘˜è¦:")
    print(f"  æ€»ç›˜æ•´åŒºåŸŸ: {summary['total_zones']}")
    print(f"  å¹³å‡æŒç»­æ—¶é—´: {summary['avg_duration']:.1f} æ ¹Kçº¿")
    print(f"  å¹³å‡æ³¢åŠ¨èŒƒå›´: {summary['avg_range_pct']:.2f}%")
    print(f"  å¹³å‡å¼ºåº¦å¾—åˆ†: {summary['avg_strength']:.2f}")
    print(f"  çªç ´å€¾å‘åˆ†å¸ƒ: {summary['breakout_bias_distribution']}")